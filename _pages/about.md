---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My name is Toon Tran (Tung Tran). I double major in Math and Computer Science at Bucknell University. 

I'm fortunate to be advised by the wonderful Professor [Joshua Stough](http://eg.bucknell.edu/~jvs008/research/research.html) (Bucknell University) and [Dr. Haggerty](https://www.geisinger.edu/research/research-at-geisinger/find-an-investigator/2018/04/04/13/27/christopher-m-haggerty) (Geisinger Health) in my previous research on automatic echocardiography segmentation. 

[[CV](https://drive.google.com/file/d/1M2LJpfzJuFoouyfJtw_P6ePrNk_n-4vw/view?usp=sharing)] [[Google Scholar](https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=plU7WMQAAAAJ)] [[Github](https://github.com/)]<br>
Email: *tst008 [at] bucknell [dot] edu*

<!-- Education
=== -->
<!-- <details>
  <summary><b>Older News</b></summary>

  <b>[December 2021]</b> I began as a teaching assistant in <a href="https://nasmith.github.io/NLP-winter22/about/">Natural Language Processing</a> at UW for winter and spring quarter. <br/>
</details>
<br> -->

Research
====

My current interest lies in **machine learning**, **optimization** and **deep learning**. 

Specifically, I'm looking at **theoretical deep learning** frameworks such as [Tensor Programs](https://www.microsoft.com/en-us/research/people/gregyang/), [Neural Tangent Kernels](https://en.wikipedia.org/wiki/Neural_tangent_kernel), or data-dependent two-layer networks to approach *memorization* in neural networks. Memorization is referred to here as both tendency to memorize noise (overfit) and statistical features (causing lower performance on distribution shift).   

Previously, I did research on **language models for theorem proving**. Specifically, I examined the ability of the neural network GPT-2 to incrementally learn to prove math statements from its own mistakes in Lean, an Interactive Theorem Prover for doing math on computers. Finding my model does not respond well to distribution shift, I decided to dive into the fundamentals of deep learning instead. This topic is still in my interest, though I will pursue it at a later time.

In another previous research, I looked into **automatic echocardiography segmentation** using CNNs. My model also exhibit memorization, making it hard to generalize across different datasets.



Publications
===

*Conference Papers*

**Bayesian Optimization of 2D Echocardiography Segmentation** <br>
**Toon Tran**, Joshua V. Stough, Xiaoyan Zhang, Christopher M. Haggerty <br>
In Proceedings of the IEEE International Symposium on Biomedical Imaging (**ISBI '21**) <br>
[[arxiv](https://arxiv.org/abs/2211.09888)] [[poster](http://eg.bucknell.edu/~jvs008/research/cardiac/ISBI21/tranISBI21_poster.pdf)] [[video](https://www.youtube.com/watch?v=l6G8El8_X4o)]
<br><br>

*Technical Reports and Preprints*

**Symmetric Diffeomorphic Registration as Segmentation Post-processing Method** <br>
**Toon Tran**, Joshua V. Stough <br>
Technical Report <br>
[[arxiv]()]


Miscellaneous
===

- I enjoy reading comics, fantasy and sci-fi books. I also play badminton casually when I get the chance.
- If you want to discuss or study epistemology/philosphy of science, do hit me up.
- Please feel free to [schedule a meeting](https://go.oncehub.com/ToonTran) or send me an email if you have any question about anything, or if you just want to have a chat!
